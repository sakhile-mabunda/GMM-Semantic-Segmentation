{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from glob import glob\n",
    "from PIL import Image  \n",
    "from torchsummary import summary\n",
    "from urllib.request import urlretrieve\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "np.set_printoptions(threshold = np.inf) \n",
    "np.set_printoptions(suppress = True)\n",
    "\n",
    "cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters etc.\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "IMAGE_HEIGHT = 512  \n",
    "IMAGE_WIDTH = 512  \n",
    "# PIN_MEMORY = True\n",
    "\n",
    "# os.makedirs(\"saved_images\", exist_ok=True)\n",
    "epochs = 16\n",
    "batch_size = 2\n",
    "lr = 0.0001\n",
    "img_scale= 0.1,\n",
    "out_channel= 1\n",
    "pretrained = False\n",
    "load_model = False\n",
    "dir_checkpoint = 'checkpoints/'\n",
    "os.makedirs(\"saved_images\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_directory = 'images-1024x768'\n",
    "masks_directory = 'masks-1024x768'\n",
    "images_filenames = list(sorted(os.listdir(images_directory)))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(images_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinoDataset(Dataset):\n",
    "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None,mode = 'train' ):\n",
    "\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.masks_directory = masks_directory\n",
    "        self.transform = transform\n",
    "        \n",
    "        len_training =  int(len(images_filenames) * (70/100))\n",
    "        len_val = int(len(images_filenames) * (90/100))\n",
    "\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            self.images_filenames = self.images_filenames[:len_training] \n",
    "        elif mode == \"val\":\n",
    "            self.images_filenames = self.images_filenames[len_training:len_val]\n",
    "        else :\n",
    "            self.images_filenames = self.images_filenames[len_val:] \n",
    "\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "        \n",
    "        image_filename = self.images_filenames[idx]\n",
    "        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(\n",
    "            os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,\n",
    "        )\n",
    "\n",
    "\n",
    "       \n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "train_dataset = TinoDataset(images_filenames=images_filenames, images_directory=images_directory , masks_directory=masks_directory, transform=train_transform,)\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [A.Resize(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n",
    ")\n",
    "val_dataset = TinoDataset(images_filenames=images_filenames, images_directory=images_directory , masks_directory=masks_directory, transform=val_transform, mode = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(dataset, idx=0, samples=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    \n",
    "    figure, ax = plt.subplots(nrows=5, ncols=2, figsize=(10, 24))\n",
    "   \n",
    "    for i in range(samples):\n",
    "        image, mask = dataset[idx]\n",
    "        ax[i, 0].imshow(image)\n",
    "        ax[i, 1].imshow(mask, cmap =  'gray',interpolation=\"nearest\")\n",
    "        ax[i, 0].set_title(\"Augmented image\")\n",
    "        ax[i, 1].set_title(\"Augmented mask\")\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "visualize_augmentations(train_dataset, idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3,1, 1,bias=False)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_: int, out: int):\n",
    "        super(ConvRelu, self).__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
    "    link https://distill.pub/2016/deconv-checkerboard/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_deconv:\n",
    "            self.block = nn.Sequential(\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                \n",
    "                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=2, stride=2\n",
    "                                   ),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class VGG16UNet(nn.Module):\n",
    "    def __init__(self, out_channel=1, num_filters=32, pretrained=False):\n",
    "        \"\"\"\n",
    "        out_channel:Number of channels of output image\n",
    "        pretrained: Whether to load the pre-trained model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.out_channel = out_channel\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = torchvision.models.vgg16(pretrained=pretrained).features#The pre-trained model will be downloaded automatically\n",
    "        print(len(self.encoder))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.encoder_conv1_1=nn.Conv2d(out_channel, 64, 3,1,  1)#Modify the first layer of VGG16 to use single-channel images, this part of the parameters.\n",
    "        '''\n",
    "        self.conv1 = nn.Sequential(self.encoder_conv1_1,\n",
    "                                   self.relu,\n",
    "                                   self.encoder[2],\n",
    "                                   self.relu)\n",
    "        '''\n",
    "        self.conv1 = nn.Sequential(self.encoder[0],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[2],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv2 = nn.Sequential(self.encoder[5],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[7],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv3 = nn.Sequential(self.encoder[10],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[12],\n",
    "                                   self.relu,\n",
    "                                #    self.encoder[14],\n",
    "                                #    self.relu\n",
    "                                )\n",
    "\n",
    "        self.conv4 = nn.Sequential(self.encoder[17],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[19],\n",
    "                                   self.relu,\n",
    "                                #    self.encoder[21],\n",
    "                                #    self.relu\n",
    "                                )\n",
    "\n",
    "        self.conv5 = nn.Sequential(self.encoder[24],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[26],\n",
    "                                   self.relu,\n",
    "                                #    self.encoder[28],\n",
    "                                #    self.relu\n",
    "                                   )\n",
    "\n",
    "        self.center = DecoderBlock(512, num_filters * 8 * 2, num_filters * 8)\n",
    "\n",
    "        self.dec5 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec4 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec3 = DecoderBlock(256 + num_filters * 8, num_filters * 4 * 2, num_filters * 2)\n",
    "        self.dec2 = DecoderBlock(128 + num_filters * 2, num_filters * 2 * 2, num_filters)\n",
    "        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, out_channel, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        skip_connections = []\n",
    "        \n",
    "\n",
    "        conv1 = self.conv1(x)\n",
    "        skip_connections.append(conv1)\n",
    "\n",
    "        conv2 = self.conv2(conv1)\n",
    "        skip_connections.append(conv2)\n",
    "        conv2 =  self.pool(conv2)\n",
    "\n",
    "        conv3 = self.conv3(conv2)\n",
    "        skip_connections.append(conv3)\n",
    "        conv3 =  self.pool(conv3)\n",
    "\n",
    "        conv4 = self.conv4(conv3)\n",
    "        skip_connections.append(conv4)\n",
    "        conv4 =  self.pool(conv4)\n",
    "\n",
    "        # conv5 = self.conv5(conv4)\n",
    "        # skip_connections.append(conv5)\n",
    "        # conv5 =  self.pool(conv5)\n",
    "\n",
    "        center = self.center(self.pool(conv4))\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "        \n",
    "   \n",
    "        \n",
    "\n",
    "        dec4 = self.dec4(torch.cat([center, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "        \n",
    "        x_out = self.final(dec1)\n",
    "        \n",
    "\n",
    "        return x_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_as_imgs( loader, model, folder=\"saved_images/\", device=\"cuda\"):\n",
    "    \n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device).float()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "           \n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y.unsqueeze(1).float(), f\"{folder}{idx}.png\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net,\n",
    "              device,\n",
    "              epochs=5,\n",
    "              batch_size=1,\n",
    "              lr=0.001,\n",
    "              save_cp=True,\n",
    "              img_scale=0.5,\n",
    "              out_channel=1):\n",
    "    #Data loader\n",
    "\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    #Optimizer loss function\n",
    "    optimizer = optim.Adam(net.parameters(),lr=lr)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    \n",
    "    for epoch in range(0,epochs):\n",
    "        #Adjust the learning rate\n",
    "        if epoch % 20 == 0:\n",
    "            for p in optimizer.param_groups:\n",
    "                p['lr'] *= 0.5\n",
    "\n",
    "\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        global_step=0\n",
    "        loop = tqdm(train_loader)\n",
    "        for batch , (data, targets) in enumerate( loop ):\n",
    "            #Get data and load it to CPU/GPU\n",
    "           \n",
    "            src = data.to(device=device, dtype=torch.float32)\n",
    "            target = targets.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()   \n",
    "            src_pred = net(src)#Forward results\n",
    "\n",
    "      \n",
    "            loss = criterion(src_pred, target)#Calculate the loss\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if global_step%100==0:\n",
    "                print(src_pred[0][0][0][0])\n",
    "                print('global_step:',global_step,' ; loss:',loss.item())\n",
    "            global_step+=1\n",
    "            \n",
    "            # print('-------------------------------------------epoch:',epoch,' ; loss_mean:',epoch_loss/len(train_loader))\n",
    "\n",
    "            #Save the model    \n",
    "            if save_cp and epoch%10==0:\n",
    "                try:\n",
    "                    os.mkdir(dir_checkpoint)\n",
    "                    print('Created checkpoint directory')\n",
    "                except OSError:\n",
    "                    pass\n",
    "                torch.save(net.state_dict(),\n",
    "                            dir_checkpoint + f'Checkpoint_epoch{epoch}.pth')\n",
    "                print(f'Checkpoint {epoch} saved !')\n",
    "                save_predictions_as_imgs( val_loader, net, folder=\"saved_images/\", device=\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#The parameters of the pre-trained model can be automatically loaded：Copy the pre-downloaded model directly to the cache directory to save temporary download time.\n",
    "net = VGG16UNet( out_channel=out_channel, pretrained=pretrained )\n",
    "\n",
    "#initialization Conv2d,ReLU,MaxPool2d,ConvTranspose2d,\n",
    "num=0\n",
    "for m in net.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    num+=1\n",
    "\n",
    "            \n",
    "#Load the parameters of the last training\n",
    "if load_model :\n",
    "    net.load_state_dict(torch.load('Checkpoint_epoch100l1.pth'))\n",
    "\n",
    "\n",
    "#Load data to CPU/GPU\n",
    "\n",
    "net.to(device=device,dtype=torch.float32)\n",
    "#summary(net, (1, 224, 224)) \n",
    "\n",
    "train_net(net=net,\n",
    "            epochs= epochs,\n",
    "            batch_size = batch_size,\n",
    "            lr = lr,\n",
    "            device=device,\n",
    "            img_scale= img_scale,\n",
    "            out_channel= out_channel\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18cb3372273ceebc5cd984fea0e9e91c601bb4562d51aa4cc88cd04c729e11a3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('pytorch_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
